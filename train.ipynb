{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Specifics:\n",
    "- We're defining a feed-forawrd NN with four layers (input layer, two hidden layers, and output layer).\n",
    "- Input layer is each of the 784 grayscale values\n",
    "- The hidden layers will have sixteen neurons each.\n",
    "- Activation functions of hidden layer neurons will be the ReLU\n",
    "- There will be 10 output layer neurons, representing the computed probability of the digit the input image represents.\n",
    "- Activation functions of the output layer will be softmax\n",
    "- We'll use cross entropy as the loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(\"mnist_train.csv\").to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise weight matrices and bias vectors using random normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    z=0\n",
    "    a=0\n",
    "    def __init__(self, dimensions):\n",
    "        self.weights=np.random.normal(loc=0,scale=0.01,size=dimensions)\n",
    "        self.biases=np.random.normal(loc=0, scale=0.01, size=dimensions[1])\n",
    "\n",
    "#Layer 1\n",
    "l1=Layer((784,16))\n",
    "\n",
    "#Layer 2\n",
    "l2=Layer((16,16))\n",
    "\n",
    "#Layer 3\n",
    "l3=Layer((16,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def softmax(x):\n",
    "    m=np.max(x)\n",
    "    return np.exp(x-m)/np.exp(x-m).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(training_data):\n",
    "    l1.z=np.dot(training_data[1:],l1.weights)+l1.biases\n",
    "    l1.a=ReLU(l1.z)\n",
    "\n",
    "    l2.z=np.dot(l1.z, l2.weights)+l2.biases\n",
    "    l2.a=ReLU(l2.z)\n",
    "\n",
    "    l3.z=np.dot(l2.z, l3.weights)+l3.biases\n",
    "    l3.a=softmax(l3.z)\n",
    "    return l1, l2, l3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(training_data):\n",
    "    y=np.zeros(10)\n",
    "    index=training_data[0]\n",
    "    y[index]=1\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross-entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(training_data, predictions):\n",
    "    epsilon = 1e-10\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    y=one_hot(training_data)\n",
    "    ce= -np.mean(np.log(predictions)*y)\n",
    "    return ce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dReLU(z):\n",
    "    return z>0\n",
    "\n",
    "def backword_prop(training_data, l1,l2,l3,y):\n",
    "    dZ3=l3.a-y\n",
    "    dZ2=np.dot(l3.weights, l3.a-y) * dReLU(l2.z)\n",
    "    dZ1=np.dot(l2.weights, dZ2) * dReLU(l1.z)\n",
    "    dW3 = np.outer(l2.a, dZ3)\n",
    "    dW2 = np.outer(l1.a, dZ2)\n",
    "    dW1 = np.outer(training_data[1:], dZ1)\n",
    "    dB1 = dZ1\n",
    "    dB2=dZ2\n",
    "    dB3= dZ3\n",
    "\n",
    "    return dW1, dW2, dW3, dB1, dB2, dB3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2409\n",
      "Epoch: 1, Loss: 2409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m training_data \u001b[39min\u001b[39;00m dataset_train:\n\u001b[0;32m---> 13\u001b[0m     l1,l2,l3\u001b[39m=\u001b[39mforward_prop(training_data)\n\u001b[1;32m     14\u001b[0m     y\u001b[39m=\u001b[39mone_hot(training_data)\n\u001b[1;32m     15\u001b[0m     dW1, dW2, dW3, dB1, dB2, dB3\u001b[39m=\u001b[39mbackword_prop(training_data, l1,l2,l3,y)\n",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop\u001b[39m(training_data):\n\u001b[0;32m----> 2\u001b[0m     l1\u001b[39m.\u001b[39mz\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mdot(training_data[\u001b[39m1\u001b[39;49m:],l1\u001b[39m.\u001b[39;49mweights)\u001b[39m+\u001b[39ml1\u001b[39m.\u001b[39mbiases\n\u001b[1;32m      3\u001b[0m     l1\u001b[39m.\u001b[39ma\u001b[39m=\u001b[39mReLU(l1\u001b[39m.\u001b[39mz)\n\u001b[1;32m      5\u001b[0m     l2\u001b[39m.\u001b[39mz\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdot(l1\u001b[39m.\u001b[39mz, l2\u001b[39m.\u001b[39mweights)\u001b[39m+\u001b[39ml2\u001b[39m.\u001b[39mbiases\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "epochs=1000\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(dataset_train)\n",
    "    gradient = {\"dW1\": np.zeros(l1.weights.shape), \n",
    "                \"dB1\": np.zeros(l1.biases.shape),\n",
    "                \"dW2\": np.zeros(l2.weights.shape), \n",
    "                \"dB2\": np.zeros(l2.biases.shape), \n",
    "                \"dW3\": np.zeros(l3.weights.shape), \n",
    "                \"dB3\": np.zeros(l3.biases.shape)}\n",
    "    loss=0\n",
    "    for training_data in dataset_train:\n",
    "        l1,l2,l3=forward_prop(training_data)\n",
    "        y=one_hot(training_data)\n",
    "        dW1, dW2, dW3, dB1, dB2, dB3=backword_prop(training_data, l1,l2,l3,y)\n",
    "        gradient[\"dW1\"]=np.add(gradient[\"dW1\"],dW1)\n",
    "        gradient[\"dW2\"]=np.add(gradient[\"dW2\"],dW2)\n",
    "        gradient[\"dW3\"]=np.add(gradient[\"dW3\"],dW3)\n",
    "\n",
    "        gradient[\"dB1\"]=np.add(gradient[\"dB1\"],dB1)\n",
    "        gradient[\"dB2\"]=np.add(gradient[\"dB2\"],dB2)\n",
    "        gradient[\"dB3\"]=np.add(gradient[\"dB3\"],dB3)\n",
    "        loss+=ce_loss(training_data, l3.a)\n",
    "\n",
    "    for array in gradient.values():\n",
    "        array/=dataset_train.shape[0]\n",
    "    loss\n",
    "    l1.weights -= learning_rate * gradient[\"dW1\"]\n",
    "    l1.biases -= learning_rate * gradient[\"dB1\"]\n",
    "    l2.weights -= learning_rate * gradient[\"dW2\"]\n",
    "    l2.biases -= learning_rate * gradient[\"dB2\"]\n",
    "    l3.weights -= learning_rate * gradient[\"dW3\"]\n",
    "    l3.biases -= learning_rate * gradient[\"dB3\"]\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %d\"%(epoch, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value:  0\n"
     ]
    }
   ],
   "source": [
    "index=3\n",
    "dataset_test = pd.read_csv(\"mnist_test.csv\").to_numpy()\n",
    "plt.imshow(dataset_test[index][1:].reshape(28,28),cmap=\"gray\")\n",
    "l1,l2,l3 = forward_prop(dataset_test[index])\n",
    "print(\"input:\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Predicted value: \",np.argmax(l3.a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.06\n"
     ]
    }
   ],
   "source": [
    "sucesses=0\n",
    "for test_data in dataset_test:\n",
    "    l1,l2,l3 = forward_prop(test_data)\n",
    "    if np.argmax(l3.a)==test_data[0]:\n",
    "        sucesses+=1\n",
    "print(sucesses/dataset_test.shape[0]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
