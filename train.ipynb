{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Specifics:\n",
    "- We're defining a feed-forawrd NN with four layers (input layer, two hidden layers, and output layer).\n",
    "- Input layer is each of the 784 grayscale values\n",
    "- The hidden layers will have sixteen neurons each.\n",
    "- Activation functions of hidden layer neurons will be the ReLU\n",
    "- There will be 10 output layer neurons, representing the computed probability of the digit the input image represents.\n",
    "- Activation functions of the output layer will be softmax\n",
    "- We'll use cross entropy as the loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(\"mnist_train.csv\").to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise weight matrices and bias vectors using random normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    z=0\n",
    "    a=0\n",
    "    def __init__(self, dimensions):\n",
    "        self.weights=np.random.normal(loc=0,scale=0.01,size=dimensions)\n",
    "        self.biases=np.random.normal(loc=0, scale=0.01, size=dimensions[1])\n",
    "\n",
    "#Layer 1\n",
    "l1=Layer((784,16))\n",
    "\n",
    "#Layer 2\n",
    "l2=Layer((16,16))\n",
    "\n",
    "#Layer 3\n",
    "l3=Layer((16,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def softmax(x):\n",
    "    m=np.max(x)\n",
    "    return np.exp(x-m)/np.exp(x-m).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(training_data):\n",
    "    l1.z=np.dot(training_data[1:],l1.weights)+l1.biases\n",
    "    l1.a=ReLU(l1.z)\n",
    "\n",
    "    l2.z=np.dot(l1.z, l2.weights)+l2.biases\n",
    "    l2.a=ReLU(l2.z)\n",
    "\n",
    "    l3.z=np.dot(l2.z, l3.weights)+l3.biases\n",
    "    l3.a=softmax(l3.z)\n",
    "    return l1, l2, l3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(training_data):\n",
    "    y=np.zeros(10)\n",
    "    index=training_data[0]\n",
    "    y[index]=1\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross-entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(training_data, predictions):\n",
    "    epsilon = 1e-10\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    y=one_hot(training_data)\n",
    "    ce= -np.mean(np.log(predictions)*y)\n",
    "    return ce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dReLU(z):\n",
    "    return z>0\n",
    "\n",
    "def backword_prop(training_data, l1,l2,l3,y):\n",
    "    dZ3=l3.a-y\n",
    "    dZ2=np.dot(l3.weights, l3.a-y) * dReLU(l2.z)\n",
    "    dZ1=np.dot(l2.weights, dZ2) * dReLU(l1.z)\n",
    "    dW3 = np.outer(l2.a, dZ3)\n",
    "    dW2 = np.outer(l1.a, dZ2)\n",
    "    dW1 = np.outer(training_data[1:], dZ1)\n",
    "    dB1 = dZ1\n",
    "    dB2=dZ2\n",
    "    dB3= dZ3\n",
    "\n",
    "    return dW1, dW2, dW3, dB1, dB2, dB3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2409\n",
      "Epoch: 1, Loss: 2409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m training_data \u001b[39min\u001b[39;00m dataset_train:\n\u001b[0;32m---> 13\u001b[0m     l1,l2,l3\u001b[39m=\u001b[39mforward_prop(training_data)\n\u001b[1;32m     14\u001b[0m     y\u001b[39m=\u001b[39mone_hot(training_data)\n\u001b[1;32m     15\u001b[0m     dW1, dW2, dW3, dB1, dB2, dB3\u001b[39m=\u001b[39mbackword_prop(training_data, l1,l2,l3,y)\n",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop\u001b[39m(training_data):\n\u001b[0;32m----> 2\u001b[0m     l1\u001b[39m.\u001b[39mz\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mdot(training_data[\u001b[39m1\u001b[39;49m:],l1\u001b[39m.\u001b[39;49mweights)\u001b[39m+\u001b[39ml1\u001b[39m.\u001b[39mbiases\n\u001b[1;32m      3\u001b[0m     l1\u001b[39m.\u001b[39ma\u001b[39m=\u001b[39mReLU(l1\u001b[39m.\u001b[39mz)\n\u001b[1;32m      5\u001b[0m     l2\u001b[39m.\u001b[39mz\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdot(l1\u001b[39m.\u001b[39mz, l2\u001b[39m.\u001b[39mweights)\u001b[39m+\u001b[39ml2\u001b[39m.\u001b[39mbiases\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "epochs=1000\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(dataset_train)\n",
    "    gradient = {\"dW1\": np.zeros(l1.weights.shape), \n",
    "                \"dB1\": np.zeros(l1.biases.shape),\n",
    "                \"dW2\": np.zeros(l2.weights.shape), \n",
    "                \"dB2\": np.zeros(l2.biases.shape), \n",
    "                \"dW3\": np.zeros(l3.weights.shape), \n",
    "                \"dB3\": np.zeros(l3.biases.shape)}\n",
    "    loss=0\n",
    "    for training_data in dataset_train:\n",
    "        l1,l2,l3=forward_prop(training_data)\n",
    "        y=one_hot(training_data)\n",
    "        dW1, dW2, dW3, dB1, dB2, dB3=backword_prop(training_data, l1,l2,l3,y)\n",
    "        gradient[\"dW1\"]=np.add(gradient[\"dW1\"],dW1)\n",
    "        gradient[\"dW2\"]=np.add(gradient[\"dW2\"],dW2)\n",
    "        gradient[\"dW3\"]=np.add(gradient[\"dW3\"],dW3)\n",
    "\n",
    "        gradient[\"dB1\"]=np.add(gradient[\"dB1\"],dB1)\n",
    "        gradient[\"dB2\"]=np.add(gradient[\"dB2\"],dB2)\n",
    "        gradient[\"dB3\"]=np.add(gradient[\"dB3\"],dB3)\n",
    "        loss+=ce_loss(training_data, l3.a)\n",
    "\n",
    "    for array in gradient.values():\n",
    "        array/=dataset_train.shape[0]\n",
    "    loss\n",
    "    l1.weights -= learning_rate * gradient[\"dW1\"]\n",
    "    l1.biases -= learning_rate * gradient[\"dB1\"]\n",
    "    l2.weights -= learning_rate * gradient[\"dW2\"]\n",
    "    l2.biases -= learning_rate * gradient[\"dB2\"]\n",
    "    l3.weights -= learning_rate * gradient[\"dW3\"]\n",
    "    l3.biases -= learning_rate * gradient[\"dB3\"]\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %d\"%(epoch, loss))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run custom tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value:  4\n"
     ]
    }
   ],
   "source": [
    "index=24\n",
    "dataset_test = pd.read_csv(\"mnist_test.csv\").to_numpy()\n",
    "plt.imshow(dataset_test[index][1:].reshape(28,28),cmap=\"gray\")\n",
    "l1,l2,l3 = forward_prop(dataset_test[index])\n",
    "print(\"input:\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Predicted value: \",np.argmax(l3.a))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine sucess rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.06\n"
     ]
    }
   ],
   "source": [
    "sucesses=0\n",
    "for test_data in dataset_test:\n",
    "    l1,l2,l3 = forward_prop(test_data)\n",
    "    if np.argmax(l3.a)==test_data[0]:\n",
    "        sucesses+=1\n",
    "print(sucesses/dataset_test.shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"model-parameters/layer1-weights.csv\", l1.weights, delimiter=',')\n",
    "np.savetxt(\"model-parameters/layer2-weights.csv\", l2.weights, delimiter=',')\n",
    "np.savetxt(\"model-parameters/layer3-weights.csv\", l3.weights, delimiter=',')\n",
    "np.savetxt(\"model-parameters/layer1-biases.csv\", l1.biases, delimiter=',')\n",
    "np.savetxt(\"model-parameters/layer2-biases.csv\", l2.biases, delimiter=',')\n",
    "np.savetxt(\"model-parameters/layer3-biases.csv\", l3.biases, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
